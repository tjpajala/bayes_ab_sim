{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bayesian A/B simulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose you have N consumers in a test panel choosing between options A and B. You want to infer from this data which option is really the most popular option.\n",
    "\n",
    "This notebook explores the following question: can we save on A/B testing by using bayesian sequential testing - and how much?\n",
    "\n",
    "The setup is very simple:\n",
    "* generate some fake A/B choice data\n",
    "* form a Beta prior that describes your belief about the probability of A being the more chosen option \n",
    "* with each step of M data points, update your posterior to reflect the data\n",
    "* define your desired certainty level, and calculate the highest probability density interval (HPDI) of the posterior\n",
    "* if HPDI is outside the region of practical equivalence (ROPE), discontinue testing and choose the more likely alternative\n",
    "* if HPDI is completely inside ROPE, discontinue test and infer that A and B are equally good"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro.optim as optim\n",
    "from numpyro.infer import SVI, Trace_ELBO\n",
    "from numpyro.infer.autoguide import AutoLaplaceApproximation\n",
    "import arviz as az\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.faker import create_fake_ab_test_results\n",
    "from modules.utils import f_symlog\n",
    "from modules.bayes import sim_many_experiments, sim_single_experiment\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example of one experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate fake A/B data with the faker module."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_resp = 1000\n",
    "prob_a = 0.6\n",
    "results = create_fake_ab_test_results(n=n_resp, prob_a=prob_a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(Counter(results[\"choice\"]))\n",
    "print(\"Percentage of A: {}\".format(sum(results[\"choice\"]==0)/len(results[\"choice\"])))\n",
    "print(\"Prob A was set at: {}\".format(prob_a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this simple case of a binary choice experiment, we can solve the posterior analytically for the seen data:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "L = sum(results[\"choice\"]) # number of B in data\n",
    "W = len(results[\"choice\"]) - L # number of A in data\n",
    "print(\"Number of 0s (A) in data: {}\".format(W))\n",
    "print(\"Number of 0s (B) in data: {}\".format(L))\n",
    "x = jnp.linspace(0, 1, 101)\n",
    "#jnp.exp(dist.Beta(W + 1, L + 1).log_prob(x))\n",
    "plt.plot(x, jnp.exp(dist.Beta(W + 1, L + 1).log_prob(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the posterior shows that we can be quite sure that A is the more popular option."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make this sensitive to prior beliefs, we can set a prior that describes our prior understanding. `Beta(X,Y)` prior describes prior data where A was chosen X times and B was chosen Y times. For example, if we have very strong opinion that A and B are equally good, we could use a prior such as `Beta(1000,1000)`. On the other hand, if we are unsure and want a uncommitted prior, we can use e.g. `Beta(2,2)`. We will use a `Beta(10,10)` prior as default in the model, to reflect the understanding that differences between A and B are unlikely to be huge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "desired_certainty = 0.95\n",
    "prior = dist.Beta(10,10)\n",
    "#prior=dist.Uniform(0,1) # you could also use a flat prior, but that is quite unrealistic usually"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = jnp.linspace(0, 1, 101)\n",
    "plt.plot(x, jnp.exp(prior.log_prob(x)), \"--\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the prior `Beta(10,10)` shows that likely values are centered around 0.5, meaning A and B are equally good."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generally, we can use the module `numpyro` to compute posteriors for our Beta model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model(W, L, prior):\n",
    "    p = numpyro.sample(\"p\", prior)  # prior\n",
    "    numpyro.sample(\"W\", dist.Binomial(W + L, p), obs=W)  # binomial likelihood\n",
    "\n",
    "L = 4\n",
    "#print(\"Number of 0s (A) in data: {}\".format(W))\n",
    "W = 6\n",
    "#print(\"Number of 0s (B) in data: {}\".format(L))\n",
    "#print(\"Distribution of seen data:{}\".format(Counter(true_results[\"choice\"][0:i])))\n",
    "guide = AutoLaplaceApproximation(model)\n",
    "svi = SVI(model, guide, optim.Adam(1), Trace_ELBO(), W=W, L=L, prior=prior)\n",
    "params, losses = svi.run(random.PRNGKey(0), 10000, progress_bar=False)\n",
    "\n",
    "# display summary of quadratic approximation\n",
    "samples = guide.sample_posterior(random.PRNGKey(1), params, (10000,))\n",
    "numpyro.diagnostics.print_summary(samples, prob=desired_certainty, group_by_chain=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in this special beta-binomial case, we can also compute the posterior straight from the posterior distribution that we computed analytically!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s_beta = dist.Beta(W+10, L+10).sample(random.PRNGKey(1), (10000,))\n",
    "print(\"mean {:.2f}, std {:.2f}, median {:.2f}, 2.5% {:.2f}, 97.5% {:.2f}\".format(\n",
    "np.mean(s_beta), np.std(s_beta), np.median(s_beta), np.percentile(s_beta, 2.5), np.percentile(s_beta, 97.5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, results of the `numpyro` SVI computation and analytical solution are almost exactly the same (up to a certain level of randomness). Analytical solution - i.e. straight simulation from the posterior - is much faster, so we should use that whenever possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's define our function for analysis of the whole experiment. Inputs are:\n",
    "* `desired_certainty`: level of certainty about the inference of A vs. B\n",
    "* `true_results`: the results of the experiment\n",
    "* `true_probs_a`: The true probability of theta for generation of the data.\n",
    "* `min_n_obs`: Minimum number of observations that must be run before breaking the experiment.\n",
    "* `inference_strategy`: `\"analytical\"` for using the analytical solution, `\"svi\"` for SVI. Analytical is much faster.\n",
    "* `rope_width`: Radius of the ROPE region.\n",
    "* `null_effect`: Value of no effect (0.5 in binary choice).\n",
    "* `step`: Evaluate breaking of experiment at each `step` obs. Smaller is more accurate, but makes analysis runtime longer.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_resp = 500\n",
    "prob_a = 0.60\n",
    "certainty_level = 0.90\n",
    "results = faker.create_fake_ab_test_results(n=n_resp, prob_a=prob_a)\n",
    "res = sim_single_experiment(certainty_level, results, true_probs_a=[prob_a], step=50, print_progress=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cutoff = (res.ended==True).idxmax()\n",
    "res.iloc[(cutoff-1):(cutoff + 5)]\n",
    "cutoff_n = res.loc[(res.ended==True).idxmax(),\"n\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot how the CI of theta has evolved over the experiment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res.hvplot.line(x=\"n\", \n",
    "                y=[\"hpdi_lower\",\"hpdi_upper\",\"true_prob_a\"]) * \\\n",
    "    hv.HLine(0.5).opts(\n",
    "    opts.HLine(color='blue', line_width=1, line_dash=\"dashed\")) * \\\n",
    "    hv.VLine(cutoff_n).opts(\n",
    "    opts.VLine(color=\"red\", line_width=0.8, line_dash=\"dashed\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = pd.melt(res, id_vars=[\"n\"], value_vars=[\"hpdi_lower\",\"hpdi_upper\",\"true_prob_a\"])\n",
    "fig = sns.lineplot(data=d, x=\"n\",y=\"value\", hue=\"variable\")\n",
    "plt.axhline(0.5,linestyle='--', color=\"blue\")\n",
    "plt.axvline(cutoff_n, linestyle=\"--\", color=\"red\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating several experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extension of one experiment to several is simple. We just define a grid of parameters, run N experiments for each cell in grid, and collect the results to see how our bayesian system behaves at each parameter value.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us run our simulation for the parameter grid. Below you will see the defined parameters, and the setting of `max_n_resp`, describing maximum number of responses per experiment. The higher this number, the more chances our bayesian analysis has to break early and save on cost."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_n_resp = 500\n",
    "test_output = False\n",
    "if test_output:\n",
    "    all_results = sim_many_experiments(2, true_probs_a=[0.55, 0.65], desired_cert_list=[0.8, 0.9], n_resp=max_n_resp)\n",
    "else:\n",
    "    all_results = sim_many_experiments(1000, true_probs_a=[0.52, 0.53, 0.54, 0.55, 0.60, 0.65], \n",
    "                                       desired_cert_list=[0.8, 0.9, 0.95, 0.99], n_resp=max_n_resp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a DF that describes the result of each experiment at when it ended (whether due to bayesian cutoff or end of data)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_results.loc[all_results.stopping_reason==\"end_of_experiment\",\"conclusion\"] = \"uncertain\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cutoff_results = all_results[all_results[\"ended\"]==True].dropna(axis=0, how=\"any\").groupby(\"exp_no\").head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cutoff_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cutoff_results.groupby([\"desired_certainty\",\"true_prob_a\"])[\"conclusion\"].value_counts(dropna=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del all_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cert in sorted(cutoff_results.desired_certainty.unique()):\n",
    "    display(cutoff_results[cutoff_results.desired_certainty == cert].groupby([\"desired_certainty\",\"true_prob_a\"])[\"conclusion\"].value_counts(dropna=False).reset_index(name=\"values\").\\\n",
    "        pivot(index=[\"desired_certainty\",\"true_prob_a\"], columns=[\"conclusion\"],values=\"values\").reset_index().\\\n",
    "        hvplot(kind=\"bar\", x=\"true_prob_a\", y=[\"A\",\"B\",\"uncertain\"], \n",
    "               stacked=True, subplots=False, rot=90, title=\"Certainty: {}\".format(str(cert))))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 4,2\n",
    "for cert in sorted(cutoff_results.desired_certainty.unique()):\n",
    "    \n",
    "    d = cutoff_results[cutoff_results.desired_certainty == cert].\\\n",
    "        loc[:,[\"true_prob_a\",\"conclusion\"]]\n",
    "        #   id_vars=[\"true_prob_a\"], value_vars=[\"A\",\"B\",\"uncertain\"]).fillna(0)\n",
    "    #display(d)\n",
    "    \n",
    "    sns.histplot(data=d, x=\"true_prob_a\",hue=\"conclusion\",multiple=\"stack\")\n",
    "    plt.title(\"Certainty level: {}\".format(cert))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bars show us immediately a clear trend: as true probability increases, the number of `uncertain` conclusions decreases. This is intuitive - the stronger the true preference for A, the easier it is to detect with a fixed N of data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What about mistakes? Leaving out the uncertain conclusions, how often does our system make an erroneous bayesian inference?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = cutoff_results[cutoff_results.conclusion != \"uncertain\"].groupby([\"desired_certainty\",\"true_prob_a\"])[\"correct\"].value_counts(dropna=False, normalize=True).reset_index(name=\"value\").fillna(0)\n",
    "d=d.loc[d.correct==True,[\"desired_certainty\",\"true_prob_a\",\"value\"]]\\\n",
    "    .pivot(\"desired_certainty\",\"true_prob_a\",\"value\").sort_values(\"desired_certainty\",ascending=False)\n",
    "display(d.head(10))\n",
    "sns.heatmap(d, linewidths=0.5, cmap=\"PuOr\", center=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the figure above, we see that all versions of the model are over 97% accurate, and if true probability is over 55% then they are perfectly accurate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make a business decision on whether and how to use the system, we can analyse the impact on costs. This analysis assumes a fixed cost per response, and a very high cost on an erroneous inference between A and B. Naturally, such costs differ between use cases, and should preferably be elicited either from historical records or SMEs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cost_per_response = 1\n",
    "cost_of_wrong_ab_decision = 10**6 # million\n",
    "print(\"Cost per response {}, cost of wrong A/B decision {}\\n\".format(cost_per_response, cost_of_wrong_ab_decision))\n",
    "params = set(zip(cutoff_results.desired_certainty.values, cutoff_results.true_prob_a.values))\n",
    "params = list(sorted(params))\n",
    "#print(list(sorted(params)))\n",
    "\n",
    "cost_outcome_df = pd.DataFrame(params)#.pivot(index=0, columns=1, values=2)\n",
    "cost_outcome_df.columns = [\"desired_certainty\",\"true_prob_a\"]\n",
    "cost_outcome_df[[\"saved_sampling\",\"wrong_decisions\",\"cost_wrong_decisions\",\"mistake_prob\",\"total\"]] = np.nan\n",
    "\n",
    "for cert, true_prob in params:\n",
    "    print(\"For certainty {}, prob_a {} we got:\".format(cert, true_prob))\n",
    "    d = cutoff_results.loc[(cutoff_results.desired_certainty == cert) & (cutoff_results.true_prob_a == true_prob) & \\\n",
    "                          (cutoff_results.conclusion != \"uncertain\")]\n",
    "    uncertain_results = len(cutoff_results.loc[(cutoff_results.desired_certainty == cert) & (cutoff_results.true_prob_a == true_prob) & \\\n",
    "                          (cutoff_results.conclusion == \"uncertain\")])\n",
    "    \n",
    "    max_sample = max_n_resp\n",
    "    saved_in_sampling = sum([max_sample-x for x in d.n])\n",
    "    n_wrong_decisions = sum(d.correct == False)\n",
    "    correct_decisions = sum(d.correct == True)\n",
    "    wrong_decision_cost = cost_of_wrong_ab_decision * n_wrong_decisions\n",
    "    mistake_prob = (n_wrong_decisions) / (len(d) + uncertain_results)\n",
    "    print(\"Saved money in sampling {:d} (max sample {})\".format(saved_in_sampling, max_sample))\n",
    "    print(\"Lost in wrong_decisions {:d}, probability of mistake {:.2%}\".format(wrong_decision_cost, mistake_prob))\n",
    "    print(\"Total {:+d}\\n\".format(saved_in_sampling - wrong_decision_cost))\n",
    "    \n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob), \n",
    "                        \"saved_sampling\"]=saved_in_sampling\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob), \n",
    "                        \"uncertain_results\"]=uncertain_results\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob),\n",
    "                        \"wrong_decisions\"]=n_wrong_decisions\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob),\n",
    "                        \"correct_decisions\"]=correct_decisions\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob),\n",
    "                        \"cost_wrong_decisions\"]=wrong_decision_cost\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob),\n",
    "                        \"mistake_prob\"]=mistake_prob\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob),\n",
    "                        \"total\"] = saved_in_sampling - wrong_decision_cost\n",
    "    cost_outcome_df.loc[(cost_outcome_df.desired_certainty == cert) & (cost_outcome_df.true_prob_a == true_prob),\n",
    "                        \"save_per_exp\"] = (saved_in_sampling - wrong_decision_cost) / len(d)\n",
    "    \n",
    "    cost_outcome_df = cost_outcome_df[[\"desired_certainty\",\"true_prob_a\",\"uncertain_results\", \"correct_decisions\",\n",
    "                              \"wrong_decisions\",\"saved_sampling\",\"cost_wrong_decisions\",\"mistake_prob\",\n",
    "                              \"total\",\"save_per_exp\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(cost_outcome_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What do the numbers tell us? Well, for example with a true difference of 0.55 and certainty level 0.99, we would expect almost 250 euros benefit per each test that was run. For 1000 tests, such a model broke early 993 times, was uncertain 7 times and made an incorrect inference 0 times!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(cost_outcome_df.pivot(\"desired_certainty\",\"true_prob_a\",\"total\").\\\n",
    "            sort_values(\"desired_certainty\",ascending=False),\n",
    "            linewidths=0.5, cmap=\"PuOr\",center=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This last figure plots average cost/saving per experiment according to true probability of A being better, for various certainty levels. What becomes clear is that the system saves costs when A is in prior probable to be better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "symlog_costs=cost_outcome_df[[\"desired_certainty\",\"true_prob_a\",\"total\",\"save_per_exp\"]]\n",
    "\n",
    "symlog_costs.loc[:,[\"log_total\"]] = symlog_costs.total.apply(f_symlog)\n",
    "symlog_costs.loc[:,[\"log_save_per_exp\"]] = symlog_costs.save_per_exp.apply(f_symlog)\n",
    "\n",
    "d = pd.melt(symlog_costs, id_vars=[\"desired_certainty\",\"true_prob_a\"],value_vars=\"log_save_per_exp\")\n",
    "display(d.head(5))\n",
    "rcParams['figure.figsize'] = 6,3\n",
    "palette = sns.color_palette(\"mako_r\", d.desired_certainty.nunique())\n",
    "g_results=sns.lineplot(data=d, x=\"true_prob_a\",y=\"value\",hue=\"desired_certainty\", palette=palette)\n",
    "#g_results.set(yscale='log')\n",
    "plt.axhline(0, linestyle=\"--\", color=\"blue\")\n",
    "#g_results.set(xticks=sample_count)\n",
    "#g_results.set(xticklabels=sample_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the cost and benefit analysis, using the bayesian model makes sense if the true probability is at least 55%. When the true probability is less, then the system makes mistakes in inference, resulting in big losses as per the assumed large cost of erroneous inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (numpyro_testing)",
   "language": "python",
   "name": "pycharm-4becd52e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}